#Import mandatory packages.
import csv
import pandas as pd
import nltk
import sklearn
import string
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem.porter import PorterStemmer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import CountVectorizer
from stop_words import get_stop_words
from sklearn.cross_validation import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.cross_validation import cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn import linear_model
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn import cross_validation
import numpy as np

'''1.load the set of raw documents into notebook to create the dataset.
Load data from various csv files. I can load both data and labels directly from csv files
cause I have added label to each row when I create csv files.'''

try:
   
    df_b = pd.read_csv('business-raw.csv', header=None, sep=',', 
                 names=['label', 'description'])

    df_s = pd.read_csv('sport-raw.csv', header=None, sep=',', 
                 names=['label', 'description'])
except:
    print("Failed to get data!!!!")


#Create three dataframes. One is a collection of sport news, one is a collection of business news, And one is a collection of news from both topics.
#remove the duplicate data
df_sport=df_s.drop_duplicates()

#recreate the index
df_sport.index = range(len(df_sport))

#remove the duplicate data
df_business=df_b.drop_duplicates()

#recreate the index
df_business.index = range(len(df_business))

#create a whole corpus
corpus = df_business.append(df_sport)
corpus.index = range(len(corpus))

'''2.create a document-term matrix, using appropriate text preprocessing and term weighting steps.
Customerize a tokenizer function so that the corpus can be preprocessed, including tokenize, downcase and Porterstem.'''

#create a class for preprocessing corpus
class StemTokenizer(object):
     def __init__(self):
         self.ps = PorterStemmer()
     def __call__(self, doc):
         return [self.ps.stem(t) for t in nltk.word_tokenize(doc)] #tokenize,downcase and PorterStem each words
         
#Customerize a stopwords sets including stopwords and punctuations.

#get stopwords from stop_words package, the list including punctuations and stopwords
stop_words = list(get_stop_words('english'))

#get all punctuations
punctuations = list(string.punctuation)

#merge stopwords and punctuations and some lost punctuaions or stopwords
merged = stop_words+punctuations+["''","'s"]

#check the set
print(merged)

#Create document term matrix of token counts with all stopwords and punctuations removed for three datasets. All features are words which has been tokenized, downcase and stem and should appear in at least two documents.

#create a document term matrix vertorizer of token counts with all stopwords and punctuations removed
#All features are words which has been tokenized,downcase and stem 
#All words should appear in at least two documents
vectorizer = CountVectorizer(stop_words=merged, min_df = 2,tokenizer=StemTokenizer())


#create three document term matrix of different corpus
business_vec = vectorizer.fit_transform(df_business.description)
sport_vec = vectorizer.fit_transform(df_sport.description)
corpus_vec = vectorizer.fit_transform(corpus.description)

#see the terms of whole corpus
terms = vectorizer.get_feature_names()
print(terms)

#List five most frequent words in business topic
freqs = business_vec.sum(axis=0)

sorted_term_indexes = freqs.argsort()
sorted_term_indexes = sorted_term_indexes[0, ::-1]

terms = vectorizer.get_feature_names()
for i in range(5):
    term_index = sorted_term_indexes[0,i]
    print( terms[term_index] )
  
#List five most frequent words in sport topic
freqs = sport_vec.sum(axis=0)

sorted_term_indexes = freqs.argsort()
sorted_term_indexes = sorted_term_indexes[0, ::-1]

terms = vectorizer.get_feature_names()
for i in range(5):
    term_index = sorted_term_indexes[0,i]
    print( terms[term_index] )
    
#List five most frequent words in the whold corpus
freqs = corpus_vec.sum(axis=0)

sorted_term_indexes = freqs.argsort()
sorted_term_indexes = sorted_term_indexes[0, ::-1]

terms = vectorizer.get_feature_names()
for i in range(5):
    term_index = sorted_term_indexes[0,i]
    print( terms[term_index] )
    
'''Create document term matrix of tfidf with all stopwords and punctuations removed for three datasets. All features are words which has been tokenized, 
downcase and stem and should appear in at least two documents.'''

#create a document term matrix vertorizer of tfidf with all stopwords and punctuations removed
#All features are words which has been tokenized,downcase and stem 
#All words should appear in at least two documents
vectorizer_tf = TfidfVectorizer(stop_words=merged, min_df = 2,tokenizer=StemTokenizer())

#create three document term matrix of different corpus
business_vec_tf = vectorizer_tf.fit_transform(df_business.description)
sport_vec_tf = vectorizer_tf.fit_transform(df_sport.description)
corpus_vec_tf = vectorizer_tf.fit_transform(corpus.description)

#see the terms of whole corpus
terms = vectorizer_tf.get_feature_names()
print(terms)

#List five most important words in business topic

freqs = business_vec_tf.sum(axis=0)

sorted_term_indexes = freqs.argsort()
sorted_term_indexes = sorted_term_indexes[0, ::-1]

terms = vectorizer_tf.get_feature_names()
for i in range(5):
    term_index = sorted_term_indexes[0,i]
    print( terms[term_index] )
    
'''
3.Build a classification model using a classifier
Here we will create a classifier to predict the class of news. First to split the corpus
to train set and test set for training classifier model. Split rate is 0.2.'''

data_train, data_test, target_train, target_test=train_test_split(corpus.description, corpus.label, test_size=0.2)

#Using Knn(n=3) to train the model

#create the train dataset
vectorizer = TfidfVectorizer()
train_X = vectorizer.fit_transform(data_train)

#create the model
model = KNeighborsClassifier(n_neighbors=3)
model.fit(train_X, target_train)

#create the test dataset using the same vocabulary
test_X = vectorizer.transform(data_test)

#predict the result
predicted = model.predict(test_X)

'''
4.Test the predictions of the classification model using an appropriate evaluation strategy. Report and discuss results.
Using matrix to compare the prediction result and the ground truth.
We can find out the values on the diagonal from upper left to lower right is much bigger than the values on another diagonal 
which means the accuracy is really high.'''

cm = confusion_matrix(target_test, predicted,
labels=["business","sport"])
print(cm)

'''We can also show the precision intuitively like below. In this case the precision is relatively high which means the low false postive rate, 
namely the model performed well.'''

#show precision
precision_score(target_test, predicted, pos_label="business")

#We can also show the recall intuitively like below. In this case teh recall is relatively high which means low false negative rate, namely, the model performed well.
#show recall
recall_score(target_test, predicted, pos_label="business")


#We can also use f1-measure to evaluate the result, it is a trade-off between recall and precision. The f1 measure values from the worst 0 to the best 1. In this case the f1-measure is quite high which means the model in did well this case.
#show f1-measure score
f1_score(target_test, predicted, pos_label="business")

'''
Besides above evaluation methods, we can use k-fold cross validation to measure the model like below. Here is a customerize function of k-fold validation. The parameter are classifier,data,fold number, shuffle(True or False), 
label in sequence. It will print the accuracy of each iteration and return a list of all accuracys.'''

def kf_cross_validation(clf,dataframe,fold,shuffle,label):
    #store the historic accuracy
    accuracys=[]
    #invoke KFold to seperate the dataset
    cv = cross_validation.KFold(len(dataframe), n_folds=fold, shuffle=shuffle, random_state=None)
    for train_index, test_index in cv:
        X_train, X_test = dataframe.description[train_index], dataframe.description[test_index]
        y_train, y_test = dataframe.label[train_index], dataframe.label[test_index]
        
        #create the train dataset
        vectorizer = TfidfVectorizer()
        train_X = vectorizer.fit_transform(X_train)

        #create the model
        model = clf
        model.fit(train_X, y_train)

        #create the test dataset using the same vocabulary
        test_X = vectorizer.transform(X_test)

        #predict the result
        predicted = model.predict(test_X)
        accuracy = precision_score(y_test, predicted, pos_label=label)
        print("Accuracy:",accuracy)
        
        #append the result to array
        accuracys.append(accuracy)
        
        
    return accuracys
    
#Here we invoke the function and use knn(k=3) classifier,5 fold to evaluate the classifier.
scores = kf_cross_validation(KNeighborsClassifier(n_neighbors=3),corpus,5,True,"business")

'''We can find out from the result that the overall accuracy is too high which means the model performed well. But actually this result is a little tricky. Cause we can see some accuracy get to 1 and this is a obvious overfitting result. I think the reason is there are too many dimension.High-dimensionality may cause Data has many irrelevant features (dimensions) containing noise which leads to a poor model. 
Maybe we should process the data using feature extraction or selection.'''

#see the average accuracy produced by cross validation
np.mean(scores)
