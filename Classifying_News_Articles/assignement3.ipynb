{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student number:15203181\n",
    "Student name:Jinghui Lu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import mandatory packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import cross_validation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.load the set of raw documents into notebook to create the dataset. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load data from various csv files.\n",
    "I can load both **data and labels** directly from csv files cause I have added label to each row when I create csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "   \n",
    "    df_b = pd.read_csv('business-raw.csv', header=None, sep=',', \n",
    "                 names=['label', 'description'])\n",
    "\n",
    "    df_s = pd.read_csv('sport-raw.csv', header=None, sep=',', \n",
    "                 names=['label', 'description'])\n",
    "except:\n",
    "    print(\"Failed to get data!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three dataframes. One is a collection of sport news, one is a collection of business news,\n",
    "And one is a collection of news from both topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove the duplicate data\n",
    "df_sport=df_s.drop_duplicates()\n",
    "\n",
    "#recreate the index\n",
    "df_sport.index = range(len(df_sport))\n",
    "\n",
    "#remove the duplicate data\n",
    "df_business=df_b.drop_duplicates()\n",
    "\n",
    "#recreate the index\n",
    "df_business.index = range(len(df_business))\n",
    "\n",
    "#create a whole corpus\n",
    "corpus = df_business.append(df_sport)\n",
    "corpus.index = range(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label                                        description\n",
      "0    business  Australian entrepreneur Craig Wright has publi...\n",
      "1    business  Failed retailer BHS was 'crashed into a cliff'...\n",
      "2    business  Yahoo boss Marissa Mayer will get $54.9m (£37....\n",
      "3    business  Waiting staff should receive their tips in ful...\n",
      "4    business  High Street chain Argos is immediately recalli...\n",
      "5    business  Norway's sovereign wealth fund, the biggest of...\n",
      "6    business  Australian Treasurer Scott Morrison's budget o...\n",
      "7    business  Republican Party front-runner Donald Trump ste...\n",
      "8    business  The head of a South Korean division of a UK co...\n",
      "9    business  Leaked documents from EU-US trade talks reveal...\n",
      "10   business  Vimeo will let video-makers set up their own s...\n",
      "11   business  Japanese shares fall more than 3% at the start...\n",
      "12   business  Puerto Rico's governor says the territory will...\n",
      "13   business  Wales could have 50 vineyards in operation in ...\n",
      "14   business  Australian entrepreneur Craig Wright says he i...\n",
      "15   business  The BBC Business series My Shop visits revisit...\n",
      "16   business  South Korea is trying to make golf more afford...\n",
      "17   business  Holidaymakers spend an average of £200 more th...\n",
      "18   business  Ballet offers a new way for Nigeria's youth to...\n",
      "19   business  Despite Nigeria's economic challenges, the hos...\n",
      "20   business  With one week of polling left in the Welsh Ass...\n",
      "21   business  BBC Breakfast's Steph McGovern takes a look at...\n",
      "22   business       Five reasons we still love department stores\n",
      "23   business  The carpet seller who became a multimillionair...\n",
      "24   business             10 High Street brands that disappeared\n",
      "25   business                 Just how flexible can your job be?\n",
      "26   business   Why some are finding their pay packets shrinking\n",
      "27   business    How do people justify earning more than others?\n",
      "28   business              Is our data more secure in the cloud?\n",
      "29   business         The story behind France's economic malaise\n",
      "..        ...                                                ...\n",
      "134     sport  Republic of Ireland striker Shane Long was com...\n",
      "135     sport  Chelsea goalkeeper Thibaut Courtois says he is...\n",
      "136     sport  Pat Lam is confident 21-year-old Leinster star...\n",
      "137     sport  Leicester City have the chance of making anyth...\n",
      "138     sport  Leinster look likely to be without Cian Healy ...\n",
      "139     sport  Aidan O'Brien has an enviable record in the De...\n",
      "140     sport  Expectations at Manchester United are too high...\n",
      "141     sport  Laois senior football team manager Mick Lillis...\n",
      "142     sport  Cristiano Ronaldo has been passed fit for the ...\n",
      "143     sport  Claudio Ranieri hailed his Leicester players a...\n",
      "144     sport  There will be an all-Madrid Champions League f...\n",
      "145     sport  Former Ireland international David Corkery has...\n",
      "146     sport  Tottenham and Chelsea have both been charged w...\n",
      "147     sport  No Heretic prevailed in a thrilling finish to ...\n",
      "148     sport  Villarreal manager Marcelino tried to ramp up ...\n",
      "149     sport  Former Offaly captain Brian Carroll admits tha...\n",
      "150     sport  13-year-old Nicole Turner has claimed her seco...\n",
      "151     sport  Rory McIlroy will miss the BMW PGA Championshi...\n",
      "152     sport  Ulster's Ruan Pienaar believes that Munster wi...\n",
      "153     sport  Jurgen Klopp believes seven months of evolutio...\n",
      "154     sport  The draw has been made for the semi-finals of ...\n",
      "155     sport  Dublin manager Jim Gavin insists his side are ...\n",
      "156     sport  Bayern Munich chief executive Karl-Heinz Rumme...\n",
      "157     sport  Leading jockey Noel Fehily was airlifted to De...\n",
      "158     sport  Ireland have appointed former England batsman ...\n",
      "159     sport  Aidan O'Brien's Somehow needed all of Ryan Moo...\n",
      "160     sport  Harry Redknapp has revealed he won't relocate ...\n",
      "161     sport  Liverpool forward Divock Origi has not given u...\n",
      "162     sport  Aidan O'Brien believes US Army Ranger will hav...\n",
      "163     sport  Willie Mullins is considering sending Nichols ...\n",
      "\n",
      "[164 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#check dataset\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.create a document-term matrix, using appropriate text preprocessing and term weighting steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customerize a tokenizer function so that the corpus can be preprocessed, including **tokenize, downcase and Porterstem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a class for preprocessing corpus\n",
    "class StemTokenizer(object):\n",
    "     def __init__(self):\n",
    "         self.ps = PorterStemmer()\n",
    "     def __call__(self, doc):\n",
    "         return [self.ps.stem(t) for t in nltk.word_tokenize(doc)] #tokenize,downcase and PorterStem each words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customerize a stopwords sets including stopwords and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', \"''\", \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "#get stopwords from stop_words package, the list including punctuations and stopwords\n",
    "stop_words = list(get_stop_words('english'))\n",
    "\n",
    "#get all punctuations\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#merge stopwords and punctuations and some lost punctuaions or stopwords\n",
    "merged = stop_words+punctuations+[\"''\",\"'s\"]\n",
    "\n",
    "#check the set\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document term matrix of **token counts** with all stopwords and punctuations removed for three datasets. All features are words which has been **tokenized, downcase and stem** and should appear in at least **two documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1-1', '10', '100', '14', '2-2', '2016', '3-1', '49erfx', '``', 'access', 'accord', 'achiev', 'activ', 'admit', 'afternoon', 'ahead', 'aidan', 'alleg', 'allianz', 'alongsid', 'andrea', 'ankl', 'announc', 'annual', 'anoth', 'appl', 'appoint', 'around', 'atletico', 'australia', 'australian', 'author', 'back', 'bank', 'barclay', 'battl', 'bayern', 'bbc', 'beat', 'becaus', 'befor', 'behind', 'belfast', 'believ', 'betfr', 'bh', 'bid', 'big', 'biggest', 'bitcoin', 'boost', 'boss', 'brand', 'breath', 'breeder', 'brewster', 'brian', 'bridg', 'budget', 'busi', 'campaign', 'can', 'career', 'central', 'champion', 'championship', 'charg', 'chelsea', 'chester', 'china', 'chines', 'cian', 'citi', 'claim', 'clare', 'clash', 'classic', 'claudio', 'clearer', 'clinch', 'cloud', 'club', 'coach', 'colleagu', 'compani', 'complet', 'comprehens', 'comput', 'conduct', 'connacht', 'consum', 'continu', 'contract', 'control', 'counti', 'countri', 'craig', 'creator', 'cristiano', 'crown', 'crucibl', 'crunch', 'cup', 'curragh', 'custom', 'data', 'david', 'deal', 'debt', 'defend', 'depart', 'despit', 'ding', 'director', 'divis', 'doubl', 'draw', 'drawn', 'drop', 'dublin', 'duo', 'ea', 'earli', 'earn', 'econom', 'elect', 'end', 'entrepreneur', 'eu', 'european', 'even', 'event', 'execut', 'expect', 'expert', 'express', 'ey', 'fail', 'fall', 'fear', 'feel', 'fight', 'final', 'financ', 'financi', 'find', 'finish', 'firm', 'first', 'fit', 'five', 'follow', 'footbal', 'forc', 'form', 'former', 'forward', 'found', 'four', 'franc', 'free', 'fund', 'gaelic', 'game', 'gener', 'ger', 'get', 'giant', 'given', 'glori', 'go', 'goal', 'golf', 'got', 'great', 'grip', 'group', 'grow', 'ha', 'hail', 'hard', 'harri', 'head', 'heali', 'help', 'hi', 'high', 'hl', 'home', 'hope', 'hospit', 'huge', 'hurdl', 'hurl', 'hurler', 'incred', 'indic', 'industri', 'injuri', 'intern', 'ipc', 'iphon', 'ireland', 'irish', 'jami', 'japanes', 'job', 'john', 'junhui', 'just', 'know', 'korean', 'land', 'last', 'lead', 'leagu', 'leak', 'left', 'leg', 'leicest', 'leinster', 'let', 'level', 'light', 'like', 'liverpool', 'london', 'long', 'look', 'lose', 'loughnan', 'love', 'lower', 'made', 'madeira', 'madrid', 'maiden', 'make', 'manag', 'manchest', 'mark', 'market', 'may', 'mcilroy', 'million', 'mine', 'monday', 'month', 'moor', 'move', 'munich', 'munster', 'nakamoto', 'name', 'nation', 'need', 'new', 'next', 'nicol', 'nigeria', 'night', 'nine', \"o'brien\", 'offali', 'offer', 'offici', 'olymp', 'one', 'onli', 'open', 'oper', 'order', 'orlean', 'park', 'parti', 'pat', 'pay', 'peopl', 'perform', 'pga', 'place', 'plan', 'play', 'play-off', 'player', 'posit', 'premier', 'pressur', 'price', 'problem', 'produc', 'profit', 'properti', 'prospect', 'puerto', 'put', 'qualifi', 'race', 'ranieri', 'real', 'recal', 'recent', 'record', 'remain', 'remark', 'repay', 'replay', 'report', 'republ', 'requir', 'respons', 'retail', 'return', 'reveal', 'rico', 'rio', 'rival', 'ronaldo', 'room', 'rori', 'rule', 'safeti', 'said', 'sailor', 'sale', 'saskia', 'satoshi', 'saturday', 'say', 'seal', 'season', 'second', 'secretari', 'see', 'selbi', 'sell', 'semi-fin', 'sempl', 'send', 'seri', 'servic', 'set', 'set-up', 'shane', 'share', 'side', 'silver', 'similar', 'sinc', 'skill', 'south', 'space', 'speed', 'spend', 'sport', 'st', 'stadium', 'stamford', 'standard', 'start', 'steel', 'step', 'still', 'stock', 'store', 'street', 'striker', 'summer', 'sunday', 'supermarket', 'survey', 'swim', 'take', 'talk', 'target', 'team', 'thi', 'three', 'tidey', 'time', 'titl', 'tonight', 'top', 'tottenham', 'tour', 'trade', 'tri', 'triumph', 'trophi', 'troubl', 'tuesday', 'turn', 'turner', 'two', 'uk', 'ultim', 'unit', 'us', 'victori', 'villarr', 'violent', 'wa', 'wait', 'warn', 'waterford', 'way', 'wednesday', 'week', 'weekend', 'welsh', 'westmeath', 'will', 'win', 'winner', 'without', 'women', 'work', 'world', 'wright', 'year', 'york', 'zurich']\n"
     ]
    }
   ],
   "source": [
    "#create a document term matrix vertorizer of token counts with all stopwords and punctuations removed\n",
    "#All features are words which has been tokenized,downcase and stem \n",
    "#All words should appear in at least two documents\n",
    "vectorizer = CountVectorizer(stop_words=merged, min_df = 2,tokenizer=StemTokenizer())\n",
    "\n",
    "\n",
    "#create three document term matrix of different corpus\n",
    "business_vec = vectorizer.fit_transform(df_business.description)\n",
    "sport_vec = vectorizer.fit_transform(df_sport.description)\n",
    "corpus_vec = vectorizer.fit_transform(corpus.description)\n",
    "\n",
    "#see the terms of whole corpus\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 31)\t1\n",
      "  (0, 128)\t1\n",
      "  (0, 97)\t1\n",
      "  (0, 419)\t1\n",
      "  (0, 179)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 98)\t1\n",
      "  (0, 327)\t1\n",
      "  (0, 253)\t1\n",
      "  (1, 419)\t1\n",
      "  (1, 138)\t1\n",
      "  (1, 312)\t1\n",
      "  (1, 46)\t1\n",
      "  (1, 402)\t1\n",
      "  (1, 60)\t1\n",
      "  (1, 350)\t1\n",
      "  (1, 38)\t1\n",
      "  (2, 52)\t1\n",
      "  (2, 412)\t1\n",
      "  (2, 167)\t1\n",
      "  (2, 277)\t1\n",
      "  (2, 230)\t1\n",
      "  (2, 207)\t1\n",
      "  (2, 390)\t1\n",
      "  (2, 148)\t1\n",
      "  :\t:\n",
      "  (161, 169)\t1\n",
      "  (161, 41)\t1\n",
      "  (161, 331)\t1\n",
      "  (161, 313)\t1\n",
      "  (161, 157)\t1\n",
      "  (161, 190)\t1\n",
      "  (161, 22)\t1\n",
      "  (161, 199)\t1\n",
      "  (161, 226)\t1\n",
      "  (162, 412)\t1\n",
      "  (162, 362)\t1\n",
      "  (162, 186)\t1\n",
      "  (162, 398)\t1\n",
      "  (162, 373)\t1\n",
      "  (162, 303)\t1\n",
      "  (162, 25)\t1\n",
      "  (162, 44)\t1\n",
      "  (162, 17)\t1\n",
      "  (162, 263)\t1\n",
      "  (162, 157)\t1\n",
      "  (162, 69)\t1\n",
      "  (163, 4)\t1\n",
      "  (163, 243)\t1\n",
      "  (163, 339)\t1\n",
      "  (163, 193)\t1\n"
     ]
    }
   ],
   "source": [
    "#Check document term matrix of whole corpus\n",
    "print(corpus_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most frequent words in **business** topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divis\n",
      "follow\n",
      "finish\n",
      "andrea\n",
      "duo\n"
     ]
    }
   ],
   "source": [
    "freqs = business_vec.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most frequent words in **sport** topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal\n",
      "crucibl\n",
      "entrepreneur\n",
      "bank\n",
      "clearer\n"
     ]
    }
   ],
   "source": [
    "freqs = sport_vec.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most frequent words in the **whold corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ha\n",
      "hi\n",
      "leagu\n",
      "champion\n",
      "will\n"
     ]
    }
   ],
   "source": [
    "freqs = corpus_vec.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document term matrix of **tfidf** with all stopwords and punctuations removed for three datasets.\n",
    "All features are words which has been **tokenized, downcase and stem** and should appear in at least **two documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '1-1', '10', '100', '14', '2-2', '2016', '3-1', '49erfx', '``', 'access', 'accord', 'achiev', 'activ', 'admit', 'afternoon', 'ahead', 'aidan', 'alleg', 'allianz', 'alongsid', 'andrea', 'ankl', 'announc', 'annual', 'anoth', 'appl', 'appoint', 'around', 'atletico', 'australia', 'australian', 'author', 'back', 'bank', 'barclay', 'battl', 'bayern', 'bbc', 'beat', 'becaus', 'befor', 'behind', 'belfast', 'believ', 'betfr', 'bh', 'bid', 'big', 'biggest', 'bitcoin', 'boost', 'boss', 'brand', 'breath', 'breeder', 'brewster', 'brian', 'bridg', 'budget', 'busi', 'campaign', 'can', 'career', 'central', 'champion', 'championship', 'charg', 'chelsea', 'chester', 'china', 'chines', 'cian', 'citi', 'claim', 'clare', 'clash', 'classic', 'claudio', 'clearer', 'clinch', 'cloud', 'club', 'coach', 'colleagu', 'compani', 'complet', 'comprehens', 'comput', 'conduct', 'connacht', 'consum', 'continu', 'contract', 'control', 'counti', 'countri', 'craig', 'creator', 'cristiano', 'crown', 'crucibl', 'crunch', 'cup', 'curragh', 'custom', 'data', 'david', 'deal', 'debt', 'defend', 'depart', 'despit', 'ding', 'director', 'divis', 'doubl', 'draw', 'drawn', 'drop', 'dublin', 'duo', 'ea', 'earli', 'earn', 'econom', 'elect', 'end', 'entrepreneur', 'eu', 'european', 'even', 'event', 'execut', 'expect', 'expert', 'express', 'ey', 'fail', 'fall', 'fear', 'feel', 'fight', 'final', 'financ', 'financi', 'find', 'finish', 'firm', 'first', 'fit', 'five', 'follow', 'footbal', 'forc', 'form', 'former', 'forward', 'found', 'four', 'franc', 'free', 'fund', 'gaelic', 'game', 'gener', 'ger', 'get', 'giant', 'given', 'glori', 'go', 'goal', 'golf', 'got', 'great', 'grip', 'group', 'grow', 'ha', 'hail', 'hard', 'harri', 'head', 'heali', 'help', 'hi', 'high', 'hl', 'home', 'hope', 'hospit', 'huge', 'hurdl', 'hurl', 'hurler', 'incred', 'indic', 'industri', 'injuri', 'intern', 'ipc', 'iphon', 'ireland', 'irish', 'jami', 'japanes', 'job', 'john', 'junhui', 'just', 'know', 'korean', 'land', 'last', 'lead', 'leagu', 'leak', 'left', 'leg', 'leicest', 'leinster', 'let', 'level', 'light', 'like', 'liverpool', 'london', 'long', 'look', 'lose', 'loughnan', 'love', 'lower', 'made', 'madeira', 'madrid', 'maiden', 'make', 'manag', 'manchest', 'mark', 'market', 'may', 'mcilroy', 'million', 'mine', 'monday', 'month', 'moor', 'move', 'munich', 'munster', 'nakamoto', 'name', 'nation', 'need', 'new', 'next', 'nicol', 'nigeria', 'night', 'nine', \"o'brien\", 'offali', 'offer', 'offici', 'olymp', 'one', 'onli', 'open', 'oper', 'order', 'orlean', 'park', 'parti', 'pat', 'pay', 'peopl', 'perform', 'pga', 'place', 'plan', 'play', 'play-off', 'player', 'posit', 'premier', 'pressur', 'price', 'problem', 'produc', 'profit', 'properti', 'prospect', 'puerto', 'put', 'qualifi', 'race', 'ranieri', 'real', 'recal', 'recent', 'record', 'remain', 'remark', 'repay', 'replay', 'report', 'republ', 'requir', 'respons', 'retail', 'return', 'reveal', 'rico', 'rio', 'rival', 'ronaldo', 'room', 'rori', 'rule', 'safeti', 'said', 'sailor', 'sale', 'saskia', 'satoshi', 'saturday', 'say', 'seal', 'season', 'second', 'secretari', 'see', 'selbi', 'sell', 'semi-fin', 'sempl', 'send', 'seri', 'servic', 'set', 'set-up', 'shane', 'share', 'side', 'silver', 'similar', 'sinc', 'skill', 'south', 'space', 'speed', 'spend', 'sport', 'st', 'stadium', 'stamford', 'standard', 'start', 'steel', 'step', 'still', 'stock', 'store', 'street', 'striker', 'summer', 'sunday', 'supermarket', 'survey', 'swim', 'take', 'talk', 'target', 'team', 'thi', 'three', 'tidey', 'time', 'titl', 'tonight', 'top', 'tottenham', 'tour', 'trade', 'tri', 'triumph', 'trophi', 'troubl', 'tuesday', 'turn', 'turner', 'two', 'uk', 'ultim', 'unit', 'us', 'victori', 'villarr', 'violent', 'wa', 'wait', 'warn', 'waterford', 'way', 'wednesday', 'week', 'weekend', 'welsh', 'westmeath', 'will', 'win', 'winner', 'without', 'women', 'work', 'world', 'wright', 'year', 'york', 'zurich']\n"
     ]
    }
   ],
   "source": [
    "#create a document term matrix vertorizer of tfidf with all stopwords and punctuations removed\n",
    "#All features are words which has been tokenized,downcase and stem \n",
    "#All words should appear in at least two documents\n",
    "vectorizer_tf = TfidfVectorizer(stop_words=merged, min_df = 2,tokenizer=StemTokenizer())\n",
    "\n",
    "#create three document term matrix of different corpus\n",
    "business_vec_tf = vectorizer_tf.fit_transform(df_business.description)\n",
    "sport_vec_tf = vectorizer_tf.fit_transform(df_sport.description)\n",
    "corpus_vec_tf = vectorizer_tf.fit_transform(corpus.description)\n",
    "\n",
    "#see the terms of whole corpus\n",
    "terms = vectorizer_tf.get_feature_names()\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check document term matrix of tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 253)\t0.370545893725\n",
      "  (0, 327)\t0.370545893725\n",
      "  (0, 98)\t0.349257234333\n",
      "  (0, 50)\t0.349257234333\n",
      "  (0, 179)\t0.207949854156\n",
      "  (0, 419)\t0.332744467239\n",
      "  (0, 97)\t0.332744467239\n",
      "  (0, 128)\t0.349257234333\n",
      "  (0, 31)\t0.307845304147\n",
      "  (1, 38)\t0.356203021018\n",
      "  (1, 350)\t0.396669455892\n",
      "  (1, 60)\t0.318970410967\n",
      "  (1, 402)\t0.301293490377\n",
      "  (1, 46)\t0.373879941608\n",
      "  (1, 312)\t0.373879941608\n",
      "  (1, 138)\t0.341759925251\n",
      "  (1, 419)\t0.356203021018\n",
      "  (2, 148)\t0.358827461894\n",
      "  (2, 390)\t0.380699465867\n",
      "  (2, 207)\t0.380699465867\n",
      "  (2, 230)\t0.380699465867\n",
      "  (2, 277)\t0.34186221759\n",
      "  (2, 167)\t0.380699465867\n",
      "  (2, 412)\t0.240364239677\n",
      "  (2, 52)\t0.34186221759\n",
      "  :\t:\n",
      "  (161, 22)\t0.330314634359\n",
      "  (161, 190)\t0.330314634359\n",
      "  (161, 157)\t0.296617420115\n",
      "  (161, 313)\t0.274421632517\n",
      "  (161, 331)\t0.250893149523\n",
      "  (161, 41)\t0.330314634359\n",
      "  (161, 169)\t0.311337347436\n",
      "  (161, 127)\t0.330314634359\n",
      "  (161, 179)\t0.185372125838\n",
      "  (162, 69)\t0.316622971706\n",
      "  (162, 157)\t0.301653141808\n",
      "  (162, 263)\t0.301653141808\n",
      "  (162, 17)\t0.301653141808\n",
      "  (162, 44)\t0.289421900189\n",
      "  (162, 25)\t0.316622971706\n",
      "  (162, 303)\t0.316622971706\n",
      "  (162, 373)\t0.301653141808\n",
      "  (162, 398)\t0.262220828672\n",
      "  (162, 186)\t0.193682234122\n",
      "  (162, 362)\t0.316622971706\n",
      "  (162, 412)\t0.212093130935\n",
      "  (163, 193)\t0.522338206008\n",
      "  (163, 339)\t0.492328751461\n",
      "  (163, 243)\t0.492328751461\n",
      "  (163, 4)\t0.492328751461\n"
     ]
    }
   ],
   "source": [
    "#Check tfidf matrix of whole corpus\n",
    "print(corpus_vec_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most important words in **business** topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divis\n",
      "follow\n",
      "cup\n",
      "andrea\n",
      "duo\n"
     ]
    }
   ],
   "source": [
    "freqs = business_vec_tf.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer_tf.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most important words in **sport** topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deal\n",
      "crucibl\n",
      "bank\n",
      "entrepreneur\n",
      "european\n"
     ]
    }
   ],
   "source": [
    "freqs = sport_vec_tf.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer_tf.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List five most important words in the whold **corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ha\n",
      "will\n",
      "champion\n",
      "leagu\n"
     ]
    }
   ],
   "source": [
    "freqs = corpus_vec_tf.sum(axis=0)\n",
    "\n",
    "sorted_term_indexes = freqs.argsort()\n",
    "sorted_term_indexes = sorted_term_indexes[0, ::-1]\n",
    "\n",
    "terms = vectorizer_tf.get_feature_names()\n",
    "for i in range(5):\n",
    "    term_index = sorted_term_indexes[0,i]\n",
    "    print( terms[term_index] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Build a classification model using a classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a **classifier** to predict the class of news.\n",
    "First to split the corpus to train set and test set for training classifier model. Split rate is 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test, target_train, target_test=train_test_split(corpus.description, corpus.label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Knn(n=3) to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the train dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_X = vectorizer.fit_transform(data_train)\n",
    "\n",
    "#create the model\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(train_X, target_train)\n",
    "\n",
    "#create the test dataset using the same vocabulary\n",
    "test_X = vectorizer.transform(data_test)\n",
    "\n",
    "#predict the result\n",
    "predicted = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business' 'sport' 'sport' 'business' 'business' 'sport' 'sport' 'sport'\n",
      " 'business' 'sport' 'sport' 'business' 'business' 'sport' 'sport' 'sport'\n",
      " 'sport' 'business' 'business' 'business' 'sport' 'business' 'business'\n",
      " 'business' 'sport' 'business' 'business' 'sport' 'business' 'sport'\n",
      " 'business' 'business' 'business']\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Test the predictions of the classification model using an appropriate evaluation strategy. Report and discuss results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matrix to compare the prediction result and the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out the values on the diagonal from upper left to lower right is much bigger than the values on another diagonal which means the accuracy is really high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  1]\n",
      " [ 1 14]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test, predicted,\n",
    "labels=[\"business\",\"sport\"])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show the precision intuitively like below. In this case the precision is relatively high which means\n",
    "the low false postive rate, namely the model performed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94444444444444442"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show precision\n",
    "precision_score(target_test, predicted, pos_label=\"business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show the recall intuitively like below. In this case teh recall is relatively high which means low false negative rate, namely, the model performed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94444444444444442"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show recall\n",
    "recall_score(target_test, predicted, pos_label=\"business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use f1-measure to evaluate the result, it is a trade-off between recall and precision.\n",
    "The f1 measure values from the worst 0 to the best 1.\n",
    "In this case the f1-measure is quite high which means the model in did well this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94444444444444442"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show f1-measure score\n",
    "f1_score(target_test, predicted, pos_label=\"business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides above evaluation methods, we can use **k-fold cross validation** to measure the model like below.\n",
    "Here is a customerize function of k-fold validation.\n",
    "The parameter are classifier,data,fold number, shuffle(True or False), label in sequence.\n",
    "It will print the accuracy of each iteration and return a list of all accuracys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kf_cross_validation(clf,dataframe,fold,shuffle,label):\n",
    "    #store the historic accuracy\n",
    "    accuracys=[]\n",
    "    #invoke KFold to seperate the dataset\n",
    "    cv = cross_validation.KFold(len(dataframe), n_folds=fold, shuffle=shuffle, random_state=None)\n",
    "    for train_index, test_index in cv:\n",
    "        X_train, X_test = dataframe.description[train_index], dataframe.description[test_index]\n",
    "        y_train, y_test = dataframe.label[train_index], dataframe.label[test_index]\n",
    "        \n",
    "        #create the train dataset\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        train_X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "        #create the model\n",
    "        model = clf\n",
    "        model.fit(train_X, y_train)\n",
    "\n",
    "        #create the test dataset using the same vocabulary\n",
    "        test_X = vectorizer.transform(X_test)\n",
    "\n",
    "        #predict the result\n",
    "        predicted = model.predict(test_X)\n",
    "        accuracy = precision_score(y_test, predicted, pos_label=label)\n",
    "        print(\"Accuracy:\",accuracy)\n",
    "        \n",
    "        #append the result to array\n",
    "        accuracys.append(accuracy)\n",
    "        \n",
    "        \n",
    "    return accuracys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we invoke the function and use knn(k=3) classifier,5 fold to evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.923076923077\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.9\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.923076923077\n"
     ]
    }
   ],
   "source": [
    "scores = kf_cross_validation(KNeighborsClassifier(n_neighbors=3),corpus,5,True,\"business\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find out from the result that the overall accuracy is too high which means the model performed well.\n",
    "But actually this result is a little tricky. Cause we can see some accuracy get to 1 and this is a obvious **overfitting result**.\n",
    "I think the reason is there are too many dimension.**High-dimensionality** may cause Data has many irrelevant features\n",
    "(dimensions) containing noise which leads to a poor model.\n",
    "Maybe we should process the data using feature extraction or selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94923076923076921"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the average accuracy produced by cross validation\n",
    "np.mean(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
